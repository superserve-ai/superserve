---
title: "Tools"
description: "Run tools as distributed Ray tasks"
---

The `@superserve.tool` decorator converts Python functions into async [Ray tasks](https://docs.ray.io/en/latest/ray-core/tasks.html) that execute across your cluster.

```python
import superserve

@superserve.tool(num_cpus=2, memory="4GB")
async def analyze_data(dataset: str) -> dict:
    """Analyze data with heavy computation."""
    # This runs as a Ray task with 2 CPUs and 4GB memory
    return {"result": "analysis complete"}
```

You can also import the decorator directly:

```python
from superserve import tool

@tool(num_cpus=1)
def search(query: str) -> str:
    """Search for information."""
    return f"Results for: {query}"
```

When you call a tool, Superserve:
1. Submits the function as a Ray remote task
2. Allocates the specified resources (CPUs, GPUs, memory)
3. Returns an awaitable that resolves to the result

## Calling Tools

Tools are async and must be awaited:

```python
# Single tool call
result = await analyze_data("my_dataset")
```

## Parallel Execution

Use `asyncio.gather()` to run multiple tools in parallel:

```python
import asyncio

# These run concurrently on Ray workers
results = await asyncio.gather(
    analyze_data("dataset_1"),
    analyze_data("dataset_2"),
    analyze_data("dataset_3"),
)
```

## Wrapping Framework Tools

Use `superserve.tool()` as a wrapper to run framework tools on Ray:

```python
import superserve
from langchain_community.tools import DuckDuckGoSearchRun

# Wrap a LangChain tool for Ray execution
search = superserve.tool(DuckDuckGoSearchRun(), num_cpus=1)

# Now it executes on Ray workers
result = await search("Python tutorials")
```


## Resource Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `num_cpus` | int/float | 1 | CPU cores per task |
| `num_gpus` | int/float | 0 | GPUs per task |
| `memory` | str | None | Memory limit (e.g., "4GB", "512MB") |

<Info>
Tool descriptions for LLM schema generation come from the function's docstring.
</Info>
