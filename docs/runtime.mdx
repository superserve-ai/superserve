---
title: "Runtime"
description: "How RayAI uses Ray for distributed execution and serving"
---

RayAI leverages [Ray](https://ray.io) for distributed tool execution and [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) for deploying agents as scalable HTTP endpoints.

## Tools on Ray

The `@tool` decorator converts Python functions into Ray remote tasks that execute in parallel across your cluster.

```python
from rayai import tool

@tool(desc="Analyze data with heavy computation", num_cpus=2, memory="4GB")
def analyze_data(dataset: str) -> dict:
    # This runs as a Ray task with 2 CPUs and 4GB memory
    return {"result": "analysis complete"}
```

When you call `analyze_data()`, RayAI:
1. Submits the function as a Ray remote task
2. Allocates the specified resources (CPUs, GPUs, memory)
3. Waits for the result and returns it

### Parallel Execution

Multiple tool calls execute in parallel automatically:

```python
# These run concurrently on Ray
results = [
    analyze_data("dataset_1"),
    analyze_data("dataset_2"),
    analyze_data("dataset_3"),
]
```

### Tool Resource Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `desc` | str | Function docstring | Description for LLM schema generation |
| `num_cpus` | int | 1 | CPU cores per task |
| `num_gpus` | int | 0 | GPUs per task |
| `memory` | str | None | Memory limit (e.g., "4GB", "512MB") |

## Agents on Ray Serve

The `@agent` decorator marks a class as a servable agent with resource configuration for Ray Serve deployment.

```python
from rayai import agent

@agent(num_cpus=2, memory="4GB", num_replicas=3)
class MyAgent:
    def run(self, data: dict) -> dict:
        messages = data.get("messages", [])
        # Process messages and return response
        return {"response": "Hello!"}
```

### Agent Resource Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `num_cpus` | int | 1 | CPU cores per replica |
| `num_gpus` | int | 0 | GPUs per replica |
| `memory` | str | "2GB" | Memory per replica |
| `num_replicas` | int | 1 | Number of replicas for scaling |

## Serving Agents

Use `rayai serve` to deploy agents as HTTP endpoints:

```bash
rayai serve
```

RayAI automatically:
1. Discovers agents in the `agents/` directory
2. Initializes Ray and Ray Serve
3. Creates FastAPI endpoints for each agent

### Endpoints

Each agent gets a `/chat` endpoint:

```
POST /agents/{agent_name}/chat
```

Request body:
```json
{
  "data": {
    "messages": [{"role": "user", "content": "Hello!"}]
  },
  "session_id": "optional-session-id",
  "stream": false
}
```

### Streaming

Agents can support streaming responses by implementing additional methods:

| Stream Mode | Method Required | Response Format |
|-------------|-----------------|-----------------|
| `"text"` | `run_stream()` | Server-sent events with text chunks |
| `"events"` | `run_stream_events()` | Server-sent events with JSON objects |

```python
@agent()
class StreamingAgent:
    def run(self, data: dict) -> dict:
        return {"response": "Hello!"}

    async def run_stream(self, data: dict):
        """Yield text chunks for streaming."""
        yield "Hello"
        yield " world!"

    async def run_stream_events(self, data: dict):
        """Yield event dicts for streaming."""
        yield {"type": "start"}
        yield {"type": "token", "content": "Hello"}
        yield {"type": "end"}
```

### Serve Options

```bash
rayai serve [PROJECT_PATH] [OPTIONS]
```

| Option | Default | Description |
|--------|---------|-------------|
| `--port` | 8000 | HTTP server port |
| `--agents` | All | Comma-separated list of agents to deploy |

### Monitoring

When running, you can access:
- **Agent endpoints**: `http://localhost:8000/agents/{name}/chat`
- **Ray Dashboard**: `http://localhost:8265`

The Ray Dashboard provides visibility into:
- Cluster resources and utilization
- Task and actor status
- Logs and metrics
