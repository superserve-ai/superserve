---
title: "Superserve"
description: "Agent sandboxes with sessions, streaming, and zero-config deploys"
---

## The problem

You built an agent that works on your laptop. Now you need other people to use it.

Scaffolding a server is easy — any coding agent can generate a FastAPI wrapper in 30 seconds. The hard parts are production concerns that take weeks to harden, not minutes to scaffold:

- **Isolation** — your agent executes arbitrary code, writes files, spawns processes. One user's session must never see another's data. Getting this wrong is a security incident, not a bug.
- **State that persists** — a conversation is multi-turn. The agent creates files, builds up context, modifies its workspace. That state needs to survive across turns, across process restarts, and be there when a user comes back days later. Most sandbox platforms give you ephemeral compute — when the VM dies, everything is gone.
- **Sessions without glue** — tracking which user is talking to which agent instance, routing messages to the right sandbox, handling concurrent conversations. Boring infrastructure that every agent deployment needs and nobody wants to build.

Sandbox platforms like E2B and Modal give you isolated compute. But you still build the session layer, the streaming protocol, the deploy pipeline, and the state persistence yourself.

## The fix

```bash
pip install superserve
superserve deploy agent.py
```

Your agent is now a production API. Each session runs in its own [Firecracker microVM](https://firecracker-microvm.github.io/) with:

- **Hardware-level isolation** — separate kernel, filesystem, and network per session. Agents can execute code and use tools without risk of cross-session data leaks.
- **Durable workspace** — the agent's `/workspace` filesystem persists across turns and restarts. Resume a conversation days later and every file is exactly where the agent left it. State is the default, not an add-on.
- **Session management** — multi-turn conversation state handled by the platform. No database, no serialization logic, no routing code.
- **Real-time streaming** — responses stream back via SSE as the agent works.
- **~400ms cold starts** — pre-built sandbox templates with your dependencies baked in.

The same script that runs with `python agent.py` on your laptop runs in the cloud.

```bash
$ superserve run my-agent "Review the error handling in src/auth.ts — are we leaking stack traces?"

Analyzing src/auth.ts and src/middleware/auth.ts...

Found 2 issues:
1. Line 47: `catch(e) { res.json({ error: e.message }) }`
   Leaks internal error details to clients. Return a generic 500 and
   log the full error server-side.
2. Line 82: Bare `catch {}` swallows errors silently.
   At minimum, log the error. Better: re-throw after logging.

The rest of the error handling looks correct — login() and refreshToken()
both use structured error responses.

Completed in 3.8s
```

---

## Works with any agent framework

Superserve runs your code as-is. No SDK lock-in, no framework requirements.

<CardGroup cols={3}>
  <Card title="Claude Agent SDK">
    ```bash
    superserve deploy agent.py
    ```
  </Card>
  <Card title="OpenAI Agents SDK">
    ```bash
    superserve deploy agent.py
    ```
  </Card>
  <Card title="Your own HTTP server">
    ```bash
    superserve deploy server.py --port 8000
    ```
  </Card>
</CardGroup>

Same command. Same infrastructure. Same streaming, sessions, and secrets.

---

## Roadmap

We're building the production layer for AI agents. Here's what's coming next — [tell us what matters most](mailto:founders@superserve.ai).

<CardGroup cols={2}>
  <Card title="Credential proxy" icon="shield-halved">
    HTTP requests from inside the sandbox go through a proxy that injects API keys at the network level. The agent makes authenticated requests without ever seeing the credentials — they never appear in the LLM context, logs, or tool outputs. Configure once, use everywhere.
  </Card>
  <Card title="Agent observability" icon="chart-mixed">
    Every tool call, file write, and HTTP request the agent makes — logged and queryable. Get a full execution timeline for each session, not just the chat transcript. Know exactly what your agent did and why.
  </Card>
  <Card title="Spend limits and circuit breakers" icon="brake-warning">
    Set per-session and per-agent cost caps. If an agent enters a loop making expensive API calls, the sandbox kills it before you get a surprise bill. Configurable: max turns, max duration, max API spend.
  </Card>
  <Card title="Shared filesystems across agents" icon="folder-tree">
    Mount a durable filesystem into multiple sandboxes simultaneously. Agents can share data, artifacts, and context without re-uploading files or wiring object-store plumbing. Live collaboration between agents.
  </Card>
</CardGroup>

---

<CardGroup cols={2}>
  <Card title="Quickstart" icon="bolt" href="/quickstart">
    Deploy an agent in 2 minutes
  </Card>
  <Card title="Zero-Config Deploy" icon="wand-magic-sparkles" href="/zero-config">
    How it works under the hood
  </Card>
  <Card title="SDK" icon="code" href="/sdk">
    TypeScript client, React hooks
  </Card>
  <Card title="CLI" icon="terminal" href="/cli">
    All commands and flags
  </Card>
</CardGroup>
