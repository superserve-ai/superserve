---
title: "Introduction"
description: "Workspaces for agents"
---

## Agents need more than a sandbox

Agents execute code, make HTTP requests, create files, and manage credentials. In production, every session needs isolation, persistence, and governance.

Without that:
- Credentials leak into LLM context and logs
- Sessions lose state between turns and restarts
- There's no audit trail of what the agent actually did
- Agents can access anything on the network

Building this yourself means stitching together containers, proxies, secret managers, and logging - weeks of infrastructure work per agent.

## Superserve gives every agent a governed workspace

<CardGroup cols={2}>
  <Card title="Isolated by default" icon="shield-halved">
    Every session runs in its own Firecracker microVM with a dedicated kernel. The agent gets full root access to execute code, install packages, and make HTTP requests - nothing leaks into another session.
  </Card>
  <Card title="Nothing disappears" icon="hard-drive">
    The `/workspace` filesystem survives across turns, restarts, and days. Pick up a session hours or weeks later - every file and conversation is exactly where the agent left it.
  </Card>
  <Card title="Credentials stay hidden" icon="lock">
    A credential proxy injects API keys at the network level. The agent makes authenticated requests without ever seeing the credentials - they never appear in context, logs, or tool outputs.
  </Card>
  <Card title="Full audit trail" icon="chart-mixed">
    Every tool call, file write, and HTTP request the agent makes is logged and queryable. Get a full execution timeline for each session, not just the chat transcript.
  </Card>
</CardGroup>

## Zero-config deployment

Deploy with one command. No Dockerfile, no server code, no config files.

<CodeGroup>

```bash Python
superserve deploy agent.py
```

```bash TypeScript
superserve deploy agent.ts
```

</CodeGroup>

The same script you run locally runs in production. Superserve detects your dependencies, builds the sandbox, and serves your agent over a streaming API. Cold starts take ~400ms.

## Any framework

Superserve works with any agent framework - or no framework at all.

- [Claude Agent SDK](https://docs.anthropic.com/en/docs/agents-and-tools/agent-sdk/overview)
- [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)
- [LangChain](https://www.langchain.com/) / [LangGraph](https://www.langchain.com/langgraph)
- [Mastra](https://mastra.ai/)
- [Pydantic AI](https://ai.pydantic.dev/)
- Plain stdin/stdout - any script that reads input and writes output

---

## What's coming next

[Tell us what matters most](mailto:engineering@superserve.ai).

<CardGroup cols={2}>
  <Card title="Spend limits and circuit breakers" icon="brake-warning">
    Set per-session and per-agent cost caps. If an agent enters a loop making expensive API calls, the sandbox kills it before you get a surprise bill. Configurable: max turns, max duration, max API spend.
  </Card>
  <Card title="Shared filesystems across agents" icon="folder-tree">
    Mount a durable filesystem into multiple sandboxes simultaneously. Agents can share data, artifacts, and context without re-uploading files or wiring object-store plumbing.
  </Card>
</CardGroup>

---

<CardGroup cols={2}>
  <Card title="Quickstart" icon="bolt" href="/quickstart">
    Deploy an agent in 2 minutes
  </Card>
  <Card title="Deploy" icon="wand-magic-sparkles" href="/zero-config">
    Supported languages, dependency detection, configuration
  </Card>
  <Card title="SDK" icon="code" href="/sdk">
    TypeScript client, React hooks
  </Card>
  <Card title="CLI" icon="terminal" href="/cli">
    All commands and flags
  </Card>
</CardGroup>
